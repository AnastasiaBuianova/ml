{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk import RegexpTokenizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ham</td>\n",
       "      <td>I'm gonna be home soon and i don't want to tal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>spam</td>\n",
       "      <td>SIX chances to win CASH! From 100 to 20,000 po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>spam</td>\n",
       "      <td>URGENT! You have won a 1 week FREE membership ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>spam</td>\n",
       "      <td>XXXMobileMovieClub: To use your credit, click ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ham</td>\n",
       "      <td>Oh k...i'm watching here:)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ham</td>\n",
       "      <td>Eh u remember how 2 spell his name... Yes i di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ham</td>\n",
       "      <td>Fine if thats the way u feel. Thats the way ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>spam</td>\n",
       "      <td>England v Macedonia - dont miss the goals/team...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ham</td>\n",
       "      <td>Is that seriously how you spell his name?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ham</td>\n",
       "      <td>I‘m going to try for 2 months ha ha only joking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ham</td>\n",
       "      <td>So ü pay first lar... Then when is da stock co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ham</td>\n",
       "      <td>Aft i finish my lunch then i go str down lor. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ffffffffff. Alright no way I can meet up with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ham</td>\n",
       "      <td>Just forced myself to eat a slice. I'm really ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ham</td>\n",
       "      <td>Lol your always so convincing.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ham</td>\n",
       "      <td>Did you catch the bus ? Are you frying an egg ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ham</td>\n",
       "      <td>I'm back &amp;amp; we're packing the car now, I'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ahhh. Work. I vaguely remember that! What does...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5542</th>\n",
       "      <td>ham</td>\n",
       "      <td>Armand says get your ass over to epsilon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5543</th>\n",
       "      <td>ham</td>\n",
       "      <td>U still havent got urself a jacket ah?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5544</th>\n",
       "      <td>ham</td>\n",
       "      <td>I'm taking derek &amp;amp; taylor to walmart, if I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5545</th>\n",
       "      <td>ham</td>\n",
       "      <td>Hi its in durban are you still on this number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5546</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ic. There are a lotta childporn cars then.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5547</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your contract mobile 11 Mnths? Latest Moto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5548</th>\n",
       "      <td>ham</td>\n",
       "      <td>No, I was trying it all weekend ;V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5549</th>\n",
       "      <td>ham</td>\n",
       "      <td>You know, wot people wear. T shirts, jumpers, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5550</th>\n",
       "      <td>ham</td>\n",
       "      <td>Cool, what time you think you can get here?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5551</th>\n",
       "      <td>ham</td>\n",
       "      <td>Wen did you get so spiritual and deep. That's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5552</th>\n",
       "      <td>ham</td>\n",
       "      <td>Have a safe trip to Nigeria. Wish you happines...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5553</th>\n",
       "      <td>ham</td>\n",
       "      <td>Hahaha..use your brain dear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5554</th>\n",
       "      <td>ham</td>\n",
       "      <td>Well keep in mind I've only got enough gas for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5555</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yeh. Indians was nice. Tho it did kane me off ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5556</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yes i have. So that's why u texted. Pshew...mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5557</th>\n",
       "      <td>ham</td>\n",
       "      <td>No. I meant the calculation is the same. That ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5558</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5559</th>\n",
       "      <td>ham</td>\n",
       "      <td>if you aren't here in the next  &amp;lt;#&amp;gt;  hou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5560</th>\n",
       "      <td>ham</td>\n",
       "      <td>Anything lor. Juz both of us lor.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5561</th>\n",
       "      <td>ham</td>\n",
       "      <td>Get me out of this dump heap. My mom decided t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5562</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lor... Sony ericsson salesman... I ask shuh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5563</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ard 6 like dat lor.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5564</th>\n",
       "      <td>ham</td>\n",
       "      <td>Why don't you wait 'til at least wednesday to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5565</th>\n",
       "      <td>ham</td>\n",
       "      <td>Huh y lei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5566</th>\n",
       "      <td>spam</td>\n",
       "      <td>REMINDER FROM O2: To get 2.50 pounds free call...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                            message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "5     spam  FreeMsg Hey there darling it's been 3 week's n...\n",
       "6      ham  Even my brother is not like to speak with me. ...\n",
       "7      ham  As per your request 'Melle Melle (Oru Minnamin...\n",
       "8     spam  WINNER!! As a valued network customer you have...\n",
       "9     spam  Had your mobile 11 months or more? U R entitle...\n",
       "10     ham  I'm gonna be home soon and i don't want to tal...\n",
       "11    spam  SIX chances to win CASH! From 100 to 20,000 po...\n",
       "12    spam  URGENT! You have won a 1 week FREE membership ...\n",
       "13     ham  I've been searching for the right words to tha...\n",
       "14     ham                I HAVE A DATE ON SUNDAY WITH WILL!!\n",
       "15    spam  XXXMobileMovieClub: To use your credit, click ...\n",
       "16     ham                         Oh k...i'm watching here:)\n",
       "17     ham  Eh u remember how 2 spell his name... Yes i di...\n",
       "18     ham  Fine if thats the way u feel. Thats the way ...\n",
       "19    spam  England v Macedonia - dont miss the goals/team...\n",
       "20     ham          Is that seriously how you spell his name?\n",
       "21     ham    I‘m going to try for 2 months ha ha only joking\n",
       "22     ham  So ü pay first lar... Then when is da stock co...\n",
       "23     ham  Aft i finish my lunch then i go str down lor. ...\n",
       "24     ham  Ffffffffff. Alright no way I can meet up with ...\n",
       "25     ham  Just forced myself to eat a slice. I'm really ...\n",
       "26     ham                     Lol your always so convincing.\n",
       "27     ham  Did you catch the bus ? Are you frying an egg ...\n",
       "28     ham  I'm back &amp; we're packing the car now, I'll...\n",
       "29     ham  Ahhh. Work. I vaguely remember that! What does...\n",
       "...    ...                                                ...\n",
       "5542   ham           Armand says get your ass over to epsilon\n",
       "5543   ham             U still havent got urself a jacket ah?\n",
       "5544   ham  I'm taking derek &amp; taylor to walmart, if I...\n",
       "5545   ham      Hi its in durban are you still on this number\n",
       "5546   ham         Ic. There are a lotta childporn cars then.\n",
       "5547  spam  Had your contract mobile 11 Mnths? Latest Moto...\n",
       "5548   ham                 No, I was trying it all weekend ;V\n",
       "5549   ham  You know, wot people wear. T shirts, jumpers, ...\n",
       "5550   ham        Cool, what time you think you can get here?\n",
       "5551   ham  Wen did you get so spiritual and deep. That's ...\n",
       "5552   ham  Have a safe trip to Nigeria. Wish you happines...\n",
       "5553   ham                        Hahaha..use your brain dear\n",
       "5554   ham  Well keep in mind I've only got enough gas for...\n",
       "5555   ham  Yeh. Indians was nice. Tho it did kane me off ...\n",
       "5556   ham  Yes i have. So that's why u texted. Pshew...mi...\n",
       "5557   ham  No. I meant the calculation is the same. That ...\n",
       "5558   ham                             Sorry, I'll call later\n",
       "5559   ham  if you aren't here in the next  &lt;#&gt;  hou...\n",
       "5560   ham                  Anything lor. Juz both of us lor.\n",
       "5561   ham  Get me out of this dump heap. My mom decided t...\n",
       "5562   ham  Ok lor... Sony ericsson salesman... I ask shuh...\n",
       "5563   ham                                Ard 6 like dat lor.\n",
       "5564   ham  Why don't you wait 'til at least wednesday to ...\n",
       "5565   ham                                       Huh y lei...\n",
       "5566  spam  REMINDER FROM O2: To get 2.50 pounds free call...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham               Will ü b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Из наброска кода\n",
    "messages = pd.read_csv('SMSSpamCollection', sep='\\t', names=[\"label\", \"message\"])\n",
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Задание 1(+6). Проверить, сбалансирован ли датасет (может быть, наблюдений одного класса слишком много?). Какие результаты покажет dummy classifier, который будет всем новым наблюдениям присваивать класс ham? Насколько плохо такое решение для задачи определения спама?\n",
    "Грубое решение - включить в training set только необходимое число наблюдений (примерно поровну spam и ham).\n",
    "\n",
    "Нормализовать тексты и обучить байесовскую модель (bag of words). Проверить, как влияют на результат:\n",
    "\n",
    "1) разная токенизация: в одном случае знаки препинания удалять, в другом — считать их токенами; \n",
    "\n",
    "2) лемматизация (отсутствие лемматизации, стемминг, лемматизация; инструменты можно использовать любые, например, nltk.stem); \n",
    "\n",
    "3) удаление стоп-слов, а также пороги минимальной и максимальной document frequency;\n",
    "\n",
    "4) векторизация документов (CountVectorizer vs. TfIdfVectorizer);\n",
    "\n",
    "5) что-нибудь ещё?\n",
    "\n",
    "При оценке классификатора обратите внимание на TP и FP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">ham</th>\n",
       "      <th>count</th>\n",
       "      <td>4825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>4516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">spam</th>\n",
       "      <th>count</th>\n",
       "      <td>747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        message\n",
       "label                                                          \n",
       "ham   count                                                4825\n",
       "      unique                                               4516\n",
       "      top                                Sorry, I'll call later\n",
       "      freq                                                   30\n",
       "spam  count                                                 747\n",
       "      unique                                                653\n",
       "      top     Please call our customer service representativ...\n",
       "      freq                                                    4"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Из наброска кода, тааакс посмотрим на статистичку\n",
    "messages.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Отсюда мы видим, что выборка несбалансирована - спама гораздо меньше (747), чем хама (4825). В такои случае мы должны уравнять спам и хам по количеству (хоть это и грубое решение, я что-то не могу придумать, как это можно было бы сделать по-другому, иначе результат будет нерелевантен)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "spam = messages[messages['label'] == 'spam']\n",
    "ham = messages[messages['label'] == 'ham'][:len(spam)]\n",
    "messages = pd.concat([spam, ham])\n",
    "messages['length'] = messages['message'].map(lambda text: len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Функция для токенизации, удаляющей знаки препинания. Тоже из наброска код\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    return word_tokenize(text)\n",
    "#messages.message.head().apply(tokenize_punc)\n",
    "#messages.message = messages.message.apply(tokenize_punc)\n",
    "#print(messages.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Сразу заметно, что в спаме используется очень много !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Функция для токенизации, сохраняющей знаки препинания.\n",
    "def tokenize_punc(text):\n",
    "    text = text.lower()\n",
    "    return wordpunct_tokenize(text)\n",
    "#messages.message.head().apply(tokenize_punc)\n",
    "#messages.message = messages.message.apply(tokenize_punc)\n",
    "#print(messages.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Стемматизация\n",
    "def tokenize_stem(text):\n",
    "    stems = []\n",
    "    for i in RegexpTokenizer(r'\\w+').tokenize(text):\n",
    "        stems.append(EnglishStemmer(ignore_stopwords=True).stem(i))            \n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Стоп-слова подкатили\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.50      1.00      0.67       747\n",
      "       spam       0.00      0.00      0.00       747\n",
      "\n",
      "avg / total       0.25      0.50      0.33      1494\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/NP/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Векторизуем по CountVectorizer (часть кода из наброска) и смотрим на DummyClassifier\n",
    "bow = CountVectorizer()\n",
    "bow.fit_transform(messages['message'])\n",
    "\n",
    "bowed_messages = bow.transform(messages['message'])\n",
    "dummy_model = DummyClassifier(random_state=0, strategy='most_frequent')\n",
    "dummy_model.fit(bowed_messages, messages['label'])\n",
    "print(classification_report(messages['label'], dummy_model.predict(bowed_messages)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.50      1.00      0.67       747\n",
      "       spam       0.00      0.00      0.00       747\n",
      "\n",
      "avg / total       0.25      0.50      0.33      1494\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/NP/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Далаем тоже самое только с TfidfVectorizer, смотрим на DummyClassifier в этом случае. Может быть, это влияет на результат\n",
    "bow = TfidfVectorizer()\n",
    "bow.fit_transform(messages['message'])\n",
    "\n",
    "b_messages = bow.transform(messages['message'])\n",
    "dummy_model = DummyClassifier(random_state=0, strategy='most_frequent')\n",
    "dummy_model.fit(b_messages, messages['label'])\n",
    "print(classification_report(messages['label'], dummy_model.predict(bowed_messages)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Нет, на результат это не повлияло. Но выводы по DummyClassifier сделать можно. Его точно нельзя применять в этом случае. Нам же нужно иденфицировать спам сообщения, он их не определяет вообще (f1 score, precision, recall - всё по нулям).\n",
    "\n",
    "Теперь посмотрим на Байескую модель и как на неё влияют разные условия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.97      0.99      0.98       747\n",
      "       spam       0.99      0.97      0.98       747\n",
      "\n",
      "avg / total       0.98      0.98      0.98      1494\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer и токенизация с удалением знаков\n",
    "bow = CountVectorizer(analyzer=tokenize)\n",
    "bow.fit_transform(messages['message'])\n",
    "bowed_messages = bow.transform(messages['message'])\n",
    "naive_model = MultinomialNB()\n",
    "naive_model.fit(bowed_messages, messages['label'])\n",
    "print(classification_report(messages['label'], naive_model.predict(bowed_messages)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.98      1.00      0.99       747\n",
      "       spam       1.00      0.98      0.99       747\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1494\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer и токенизация с сохранением знаков\n",
    "bow = CountVectorizer(analyzer=tokenize_punc)\n",
    "bow.fit_transform(messages['message'])\n",
    "bowed_messages = bow.transform(messages['message'])\n",
    "naive_model = MultinomialNB()\n",
    "naive_model.fit(bowed_messages, messages['label'])\n",
    "print(classification_report(messages['label'], naive_model.predict(bowed_messages)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.97      0.99      0.98       747\n",
      "       spam       0.99      0.97      0.98       747\n",
      "\n",
      "avg / total       0.98      0.98      0.98      1494\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TfidfVectorizer и токенизация с удалением знаков\n",
    "bow = TfidfVectorizer(analyzer=tokenize)\n",
    "bow.fit_transform(messages['message'])\n",
    "bowed_messages = bow.transform(messages['message'])\n",
    "naive_model = MultinomialNB()\n",
    "naive_model.fit(bowed_messages, messages['label'])\n",
    "print(classification_report(messages['label'], naive_model.predict(bowed_messages)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.98      1.00      0.99       747\n",
      "       spam       1.00      0.97      0.99       747\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1494\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TfidfVectorizer и токенизация с сохранением знаков\n",
    "bow = TfidfVectorizer(analyzer=tokenize_punc)\n",
    "bow.fit_transform(messages['message'])\n",
    "bowed_messages = bow.transform(messages['message'])\n",
    "naive_model = MultinomialNB()\n",
    "naive_model.fit(bowed_messages, messages['label'])\n",
    "print(classification_report(messages['label'], naive_model.predict(bowed_messages)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проемжуточные результаты: кажется TfidfVectorizer и CountVectorizer дают одинаковые результаты, тогда я буду использовать только второе.\n",
    "Определение спама с сохранением пунктуации как-то неправдопобоно высоко. Мне кажется, это из-за того, что в спаме очень часто повторяются одни и те же слова в конце предложения, после которых идёт !, например, я видела в нашей дате: cash! ну и так далее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.97      0.99      0.98       747\n",
      "       spam       0.99      0.97      0.98       747\n",
      "\n",
      "avg / total       0.98      0.98      0.98      1494\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer и токенизация с удалением знаков и стемматизацией\n",
    "bow = CountVectorizer(analyzer=tokenize_stem)\n",
    "bow.fit_transform(messages['message'])\n",
    "bowed_messages = bow.transform(messages['message'])\n",
    "naive_model = MultinomialNB()\n",
    "naive_model.fit(bowed_messages, messages['label'])\n",
    "print(classification_report(messages['label'], naive_model.predict(bowed_messages)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.97      0.99      0.98       747\n",
      "       spam       0.99      0.97      0.98       747\n",
      "\n",
      "avg / total       0.98      0.98      0.98      1494\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer и токенизация с удалением знаков, стемматизацией и стоп-словами\n",
    "bow = CountVectorizer(analyzer=tokenize, stop_words=stopwords)\n",
    "bow.fit_transform(messages['message'])\n",
    "bowed_messages = bow.transform(messages['message'])\n",
    "naive_model = MultinomialNB()\n",
    "naive_model.fit(bowed_messages, messages['label'])\n",
    "print(classification_report(messages['label'], naive_model.predict(bowed_messages)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Мне кажется, я что-то делаю не так, не может быть такого, чтобы у меня получались одинаковые результаты с применением разных параметров"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
